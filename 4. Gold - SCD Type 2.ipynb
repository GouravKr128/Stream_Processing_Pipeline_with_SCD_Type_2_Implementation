{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1d77cba-7cea-4a0d-8c92-5e1245d2a0a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48ed2461-92cb-442e-8e50-bdfd03be02e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create Watermark Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfdded51-b667-426b-91e1-3dce5d745050",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "if not spark.catalog.tableExists(\"project.sch.watermark_tbl\"):\n",
    "    schema = StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"last_processed_timestamp\", TimestampType(), True)\n",
    "    ])\n",
    "    \n",
    "    initial_data = [(1, None)]\n",
    "    df_init = spark.createDataFrame(initial_data, schema)\n",
    "    df_init.write.format(\"delta\").saveAsTable(\"project.sch.watermark_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e578a903-dc34-4be0-afec-30082a6359a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Read new records from silver layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4d4e36f-174b-4339-9c51-4e4b9ffb2248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ADLS configuration \n",
    "spark.conf.set(\n",
    "  \"fs.azure.account.key.2adls.dfs.core.windows.net\",\n",
    "  \"<<Access_key>>\"\n",
    ")\n",
    "\n",
    "# Paths\n",
    "silver_path = \"abfss://silver@2adls.core.windows.net/\"\n",
    "dim_patient_path = \"abfss://gold@2adls.core.windows.net/dim_patient/\"\n",
    "dim_department_path = \"abfss://gold@2adls.core.windows.net/dim_department/\"\n",
    "fact_tbl_path = \"abfss://gold@2adls.core.windows.net/fact_tbl/\"\n",
    "\n",
    "# Read new data from silver layer\n",
    "watermark_df = spark.table(\"project.sch.watermark_tbl\").filter(col(\"id\") == 1)\n",
    "last_processed_val = watermark_df.collect()[0][\"last_processed_timestamp\"]\n",
    "\n",
    "silver_df = (\n",
    "     spark.read.format(\"delta\").load(silver_path) \n",
    "    .filter((col(\"ingestion_time\") > last_processed_val) | (lit(last_processed_val).isNull()))\n",
    ")\n",
    "\n",
    "# Stop execution if no. of record is less than 10\n",
    "if silver_df.count() < 10:\n",
    "    dbutils.notebook.exit(\"Stopping execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "167ee9e1-0efa-4c20-a7d4-3c4cc18d6c26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Patient Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "656e5a6a-1932-4be6-a5f8-2398a3492adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "incoming_patient = ( silver_df.select(\"patient_id\", \"gender\", \"age\")\n",
    "                         .withColumn(\"effective_from\", current_timestamp())\n",
    "                   )\n",
    "\n",
    "# Initial Run\n",
    "if not DeltaTable.isDeltaTable(spark, dim_patient_path):\n",
    "    incoming_patient.withColumn(\"surrogate_key\", monotonically_increasing_id()) \\\n",
    "                    .withColumn(\"effective_to\", lit(None).cast(\"timestamp\")) \\\n",
    "                    .withColumn(\"is_current\", lit(True)) \\\n",
    "                    .write.format(\"delta\").mode(\"overwrite\").save(dim_patient_path)\n",
    "\n",
    "# Find & Update changed records\n",
    "incoming_patient = incoming_patient.withColumn(\n",
    "    \"_hash\",\n",
    "    F.sha2(F.concat_ws(\"||\", F.coalesce(col(\"gender\"), lit(\"NA\")), F.coalesce(col(\"age\").cast(\"string\"), lit(\"NA\"))), 256)\n",
    ")\n",
    "\n",
    "target_patient = spark.read.format(\"delta\").load(dim_patient_path).withColumn(\n",
    "    \"_target_hash\",\n",
    "    F.sha2(F.concat_ws(\"||\", F.coalesce(col(\"gender\"), lit(\"NA\")), F.coalesce(col(\"age\").cast(\"string\"), lit(\"NA\"))), 256)\n",
    ")\n",
    "\n",
    "\n",
    "changes_df = (\n",
    "        target_patient.alias(\"t\").join(\n",
    "            incoming_patient.alias(\"i\"),\n",
    "            on=\"patient_id\", \n",
    "            how=\"inner\")\n",
    "        .filter((col(\"t.is_current\") == True) & (col(\"t._target_hash\") != col(\"i._hash\")))\n",
    "        .select(col(\"t.surrogate_key\"),col(\"i.effective_from\"))\n",
    ")\n",
    "\n",
    "\n",
    "target_patient_tbl = DeltaTable.forPath(spark, dim_patient_path)\n",
    "\n",
    "target_patient_tbl.alias(\"target\").merge(\n",
    "    source = changes_df.alias(\"updates\"),\n",
    "    condition = \"target.surrogate_key = updates.surrogate_key\"\n",
    ").whenMatchedUpdate(set = {\n",
    "    \"is_current\": \"false\",\n",
    "    \"effective_to\": \"updates.effective_from\"\n",
    "}).execute()\n",
    "\n",
    "\n",
    "# Insert new and changed records\n",
    "inserts_df = incoming_patient.alias(\"i\").join(\n",
    "                target_patient.alias(\"t\"),\n",
    "                on=\"patient_id\",\n",
    "                how=\"left\")\n",
    "            .filter((col(\"t.patient_id\").isNull()) |  ((col(\"t.is_current\") == True) & (col(\"i._hash\") != col(\"t._target_hash\"))))\n",
    "            .select(col(\"i.patient_id\"),col(\"i.gender\"),col(\"i.age\"),col(\"i.effective_from\"))\n",
    "\n",
    "\n",
    "max_surrogate_key = target_patient.select(max(\"surrogate_key\")).collect()[0][0]\n",
    "\n",
    "\n",
    "inserts_df.withColumn(\"surrogate_key\", max_surrogate_key + monotonically_increasing_id() + 1) \\\n",
    "  .withColumn(\"effective_to\", lit(None).cast(\"timestamp\")) \\\n",
    "  .withColumn(\"is_current\", lit(True)) \\\n",
    "  .select(\"surrogate_key\", \"patient_id\", \"gender\", \"age\", \"effective_from\", \"effective_to\", \"is_current\")\n",
    "\n",
    "\n",
    "if inserts_df.count() > 0:\n",
    "    inserts_df.write.format(\"delta\").mode(\"append\").save(dim_patient_path)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "955ad011-b5f2-411b-8fbf-2fcf83cdae88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Department Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "438be956-1447-4fb0-8409-3ce8fbe52119",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "incoming_dept = silver_df.select(\"department\", \"hospital_id\")\n",
    "                \n",
    "# Remove duplicates\n",
    "incoming_dept = incoming_dept.dropDuplicates([\"department\", \"hospital_id\"]) \n",
    "    \n",
    "# Initial Run\n",
    "if not DeltaTable.isDeltaTable(spark, dim_department_path):\n",
    "    incoming_dept = incoming_dept.withColumn(\"surrogate_key\", monotonically_increasing_id())\n",
    "    incoming_dept.select(\"surrogate_key\", \"department\", \"hospital_id\") \\\n",
    "        .write.format(\"delta\").mode(\"overwrite\").save(dim_department_path)\n",
    "\n",
    "# Incremental run\n",
    "target_dept = spark.read.format(\"delta\").load(dim_department_path)\n",
    "\n",
    "new_records = incoming_dept.join(\n",
    "    target_dept,\n",
    "    on=[\"department\", \"hospital_id\"],\n",
    "    how=\"left_anti\"\n",
    ")\n",
    "\n",
    "max_sk = target_dept.select(max(\"surrogate_key\")).collect()[0][0]\n",
    "\n",
    "incoming_dept = incoming_dept.withColumn(\"surrogate_key\", max_sk + monotonically_increasing_id() + 1)\n",
    "\n",
    "if incoming_dept.count() > 0:\n",
    "    incoming_dept.select(\"surrogate_key\", \"department\", \"hospital_id\") \\\n",
    "        .write.format(\"delta\").mode(\"append\").save(dim_department_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65625086-e86a-47d0-90e5-05020fdce42d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Fact Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b77d0a4e-b914-4526-8c5f-f9e9b9d3e29d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read current dimension tables\n",
    "dim_patient_df = (spark.read.format(\"delta\").load(dim_patient_path)\n",
    "                  .filter(col(\"is_current\") == True)\n",
    "                  .select(col(\"surrogate_key\").alias(\"surrogate_key_patient\"), \"patient_id\", \"gender\", \"age\"))\n",
    "\n",
    "dim_dept_df = (spark.read.format(\"delta\").load(dim_department_path)\n",
    "               .select(col(\"surrogate_key\").alias(\"surrogate_key_dept\"), \"department\", \"hospital_id\"))\n",
    "\n",
    "# Build fact_df from silver_df\n",
    "fact_df = (silver_df\n",
    "             .select(\"patient_id\", \"department\", \"hospital_id\", \"admission_time\", \"discharge_time\", \"bed_id\")\n",
    "             .withColumn(\"admission_date\", to_date(\"admission_time\"))\n",
    "            )\n",
    "\n",
    "# Join to get surrogate keys\n",
    "fact_df = (fact_df\n",
    "                 .join(dim_patient_df, on=\"patient_id\", how=\"left\")\n",
    "                 .join(dim_dept_df, on=[\"department\", \"hospital_id\"], how=\"left\")\n",
    "                )\n",
    "\n",
    "# Compute metrics\n",
    "fact_df = fact_df.withColumn(\"length_of_stay_hours\",\n",
    "                        (unix_timestamp(col(\"discharge_time\")) - unix_timestamp(col(\"admission_time\"))) / 3600.0) \\\n",
    "                .withColumn(\"is_currently_admitted\", when(col(\"discharge_time\") > current_timestamp(), lit(True)).  otherwise(lit(False))) \\\n",
    "                             .withColumn(\"event_ingestion_time\", current_timestamp())\n",
    "\n",
    "# insert fact_id column\n",
    "if not DeltaTable.isDeltaTable(spark, fact_tbl_path):\n",
    "    fact_df = fact_df.withColumn(\"fact_id\", monotonically_increasing_id())\n",
    "else:\n",
    "    max_fact_id = spark.read.format(\"delta\").load(fact_tbl_path).select(max(\"fact_id\")).collect()[0][0]\n",
    "    fact_df = fact_df.withColumn(\"fact_id\", max_fact_id + monotonically_increasing_id() + 1)\n",
    "\n",
    "# Select columns\n",
    "fact_df = fact_df.select(\"fact_id\",\n",
    "    col(\"surrogate_key_patient\").alias(\"patient_sk\"),\n",
    "    col(\"surrogate_key_dept\").alias(\"department_sk\"),\n",
    "    \"admission_time\",\n",
    "    \"discharge_time\",\n",
    "    \"admission_date\",\n",
    "    \"length_of_stay_hours\",\n",
    "    \"is_currently_admitted\",\n",
    "    \"bed_id\",\n",
    "    \"event_ingestion_time\"\n",
    ")\n",
    "\n",
    "# Write to fact table\n",
    "fact_df.write.format(\"delta\").mode(\"append\").save(fact_tbl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "460b45f3-50ce-4b48-98de-c3170f6c4f54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Update Watermark table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e844a814-19df-4a29-998f-a0cf68f3b38f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Update Watermark Table\n",
    "new_max_timestamp = silver_df.select(max(\"ingestion_time\")).collect()[0][0]\n",
    "spark.sql(f\"\"\"\n",
    "            UPDATE project.sch.watermark_tbl \n",
    "            SET last_processed_timestamp = '{new_max_timestamp}' \n",
    "            WHERE id = 1\n",
    "        \"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4. Gold - SCD Type 2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
